{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added: /Users/ammaster10/Documents/SIIT/Me/PawarisPanyasombat_CV.pdf\n",
      "An error occurred: [Errno 2] No such file or directory: \"'/Users/ammaster10/Downloads/1Page.pdf'\"\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfMerger\n",
    "\n",
    "def merge_pdfs(pdf_list, output_filename):\n",
    "    \"\"\"\n",
    "    Merges multiple PDF files into a single PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_list (list): List of PDF file paths to merge.\n",
    "        output_filename (str): Path to the output merged PDF file.\n",
    "    \"\"\"\n",
    "    merger = PdfMerger()\n",
    "\n",
    "    try:\n",
    "        for pdf in pdf_list:\n",
    "            merger.append(pdf)\n",
    "            print(f\"Added: {pdf}\")\n",
    "        \n",
    "        merger.write(output_filename)\n",
    "        print(f\"Merged PDF saved as: {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        merger.close()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # List of PDF files to merge\n",
    "    pdf_files = [\"/Users/ammaster10/Documents/SIIT/Me/PawarisPanyasombat_CV.pdf\", \"'/Users/ammaster10/Downloads/1Page.pdf'\"]  \n",
    "    # Output merged PDF file\n",
    "    output_file = \"merged.pdf\"  \n",
    "    \n",
    "    merge_pdfs(pdf_files, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Number of product sold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Number of product sold'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ammaster10/Documents/Github/Year3/supply_chain_data copy.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_excel(filepath, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m product_sold \u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNumber of product sold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m costs \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCosts\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate kurtosis\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Number of product sold'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "# ชื่อไฟล์มึง\n",
    "filepath = \"/Users/ammaster10/Documents/Github/Year3/supply_chain_data copy.xlsx\"\n",
    "\n",
    "\n",
    "data =pd.read_excel(filepath, sheet_name='in')\n",
    "\n",
    "product_sold =data['Number of products sold']\n",
    "costs = data['Costs']\n",
    "\n",
    "# Calculate kurtosis\n",
    "kurt_product_sold = kurtosis(product_sold)\n",
    "kurt_costs = kurtosis(costs)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot for product sold\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(product_sold, kde=True)\n",
    "plt.title(f'Product Sold\\nKurtosis: {kurt_product_sold:.2f}')\n",
    "\n",
    "# Plot for costs\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(costs, kde=True)\n",
    "plt.title(f'Costs\\nKurtosis: {kurt_costs:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [ 2.25 -3.4 ]\n",
      "Initial bias: 2.45\n",
      "\n",
      "--- Batch Gradient Descent ---\n",
      "Data: [0, 0], Target: 0, Sigmoid: 0.9206, Error: 0.9206, Gradient(w): [0. 0.], Gradient(b): 0.0673\n",
      "Data: [0, 1], Target: 1, Sigmoid: 0.2789, Error: -0.7211, Gradient(w): [-0.    -0.145], Gradient(b): -0.145\n",
      "Data: [1, 0], Target: 1, Sigmoid: 0.991, Error: -0.009, Gradient(w): [-0.0001 -0.    ], Gradient(b): -0.0001\n",
      "Data: [1, 1], Target: 1, Sigmoid: 0.7858, Error: -0.2142, Gradient(w): [-0.0361 -0.0361], Gradient(b): -0.0361\n",
      "\n",
      "After Batch Gradient Descent:\n",
      "Weights: [ 2.2536 -3.3819]\n",
      "Bias: 2.4614\n",
      "\n",
      "--- Stochastic Gradient Descent ---\n",
      "Data: [0, 0], Target: 0, Sigmoid: 0.9206, Error: 0.9206, Gradient(w): [0. 0.], Gradient(b): 0.0673\n",
      "Weight: [ 2.25 -3.4 ], Bias: 2.44327\n",
      "Data: [0, 1], Target: 1, Sigmoid: 0.2775, Error: -0.7225, Gradient(w): [-0.     -0.1449], Gradient(b): -0.1449\n",
      "Weight: [ 2.25    -3.38551], Bias: 2.45776\n",
      "Data: [1, 0], Target: 1, Sigmoid: 0.9911, Error: -0.0089, Gradient(w): [-0.0001 -0.    ], Gradient(b): -0.0001\n",
      "Weight: [ 2.25001 -3.38551], Bias: 2.45777\n",
      "Data: [1, 1], Target: 1, Sigmoid: 0.7896, Error: -0.2104, Gradient(w): [-0.035 -0.035], Gradient(b): -0.035\n",
      "Weight: [ 2.25351 -3.38201], Bias: 2.46127\n",
      "\n",
      "After Stochastic Gradient Descent:\n",
      "Weights: [ 2.2535 -3.382 ]\n",
      "Bias: 2.4613\n",
      "\n",
      "--- Mini-Batch Gradient Descent ---\n",
      "Data: [1, 1], Target: 1, Sigmoid: 0.7858, Error: -0.2142, Gradient(w): [-0.0361 -0.0361], Gradient(b): -0.0361\n",
      "Data: [0, 1], Target: 1, Sigmoid: 0.2789, Error: -0.7211, Gradient(w): [-0.    -0.145], Gradient(b): -0.145\n",
      "Data: [0, 0], Target: 0, Sigmoid: 0.9219, Error: 0.9219, Gradient(w): [0. 0.], Gradient(b): 0.0664\n",
      "Data: [1, 0], Target: 1, Sigmoid: 0.9912, Error: -0.0088, Gradient(w): [-0.0001 -0.    ], Gradient(b): -0.0001\n",
      "\n",
      "After Mini-Batch Gradient Descent:\n",
      "Weights: [ 2.2536 -3.3819]\n",
      "Bias: 2.4615\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return round(1 / (1 + np.exp(-z)), 4)\n",
    "\n",
    "# Dataset\n",
    "D = [([0, 0], 0), ([0, 1], 1), ([1, 0], 1), ([1, 1], 1)]\n",
    "\n",
    "# Initial weights and bias\n",
    "w = np.array([2.25, -3.4])  # w1, w2\n",
    "b = 2.45  # bias\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.1\n",
    "\n",
    "# Compute gradients for one sample\n",
    "def compute_gradients(x, y, w, b):\n",
    "    z = np.dot(w, x) + b  # Linear combination\n",
    "    y_hat = sigmoid(z)    # Sigmoid activation\n",
    "    error = y_hat - y      # Error\n",
    "    # Gradients with respect to weights and bias\n",
    "    dw = error * y_hat * (1 - y_hat) * np.array(x)\n",
    "    db = error * y_hat * (1 - y_hat)\n",
    "    return np.round(dw, 4), round(db, 4), round(y_hat, 4), round(error, 4)\n",
    "\n",
    "# (a) Batch Gradient Descent\n",
    "def batch_gradient_descent(D, w, b, eta):\n",
    "    total_dw = np.zeros_like(w)\n",
    "    total_db = 0\n",
    "    print(\"\\n--- Batch Gradient Descent ---\")\n",
    "    for x, y in D:\n",
    "        dw, db, y_hat, error = compute_gradients(x, y, w, b)\n",
    "        total_dw += dw\n",
    "        total_db += db\n",
    "        print(f\"Data: {x}, Target: {y}, Sigmoid: {y_hat}, Error: {error}, Gradient(w): {dw}, Gradient(b): {db}\")\n",
    "    # Update weights and bias\n",
    "    w -= eta * total_dw\n",
    "    b -= eta * total_db\n",
    "    return np.round(w, 4), round(b, 4)\n",
    "\n",
    "# (b) Stochastic Gradient Descent\n",
    "def stochastic_gradient_descent(D, w, b, eta):\n",
    "    print(\"\\n--- Stochastic Gradient Descent ---\")\n",
    "    for x, y in D:\n",
    "        dw, db, y_hat, error = compute_gradients(x, y, w, b)\n",
    "        print(f\"Data: {x}, Target: {y}, Sigmoid: {y_hat}, Error: {error}, Gradient(w): {dw}, Gradient(b): {db}\")\n",
    "        w -= eta * dw\n",
    "        b -= eta * db\n",
    "        print(f\"Weight: {w}, Bias: {b}\")\n",
    "    return np.round(w, 4), round(b, 4)\n",
    "\n",
    "# (c) Mini-Batch Gradient Descent\n",
    "def mini_batch_gradient_descent(D, w, b, eta, batch_size=2):\n",
    "    np.random.shuffle(D)\n",
    "    print(\"\\n--- Mini-Batch Gradient Descent ---\")\n",
    "    for i in range(0, len(D), batch_size):\n",
    "        batch = D[i:i+batch_size]\n",
    "        total_dw = np.zeros_like(w)\n",
    "        total_db = 0\n",
    "        for x, y in batch:\n",
    "            dw, db, y_hat, error = compute_gradients(x, y, w, b)\n",
    "            total_dw += dw\n",
    "            total_db += db\n",
    "            print(f\"Data: {x}, Target: {y}, Sigmoid: {y_hat}, Error: {error}, Gradient(w): {dw}, Gradient(b): {db}\")\n",
    "        # Update weights and bias for the batch\n",
    "        w -= eta * total_dw\n",
    "        b -= eta * total_db\n",
    "\n",
    "        \n",
    "        \n",
    "    return np.round(w, 4), round(b, 4)\n",
    "\n",
    "# Run one iteration\n",
    "print(\"Initial weights:\", w)\n",
    "print(\"Initial bias:\", b)\n",
    "\n",
    "# Batch Gradient Descent\n",
    "w_batch, b_batch = batch_gradient_descent(D, w.copy(), b, eta)\n",
    "print(\"\\nAfter Batch Gradient Descent:\")\n",
    "print(\"Weights:\", w_batch)\n",
    "print(\"Bias:\", b_batch)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "w_sgd, b_sgd = stochastic_gradient_descent(D, w.copy(), b, eta)\n",
    "print(\"\\nAfter Stochastic Gradient Descent:\")\n",
    "print(\"Weights:\", w_sgd)\n",
    "print(\"Bias:\", b_sgd)\n",
    "\n",
    "# Mini-Batch Gradient Descent\n",
    "w_mini, b_mini = mini_batch_gradient_descent(D, w.copy(), b, eta, batch_size=2)\n",
    "print(\"\\nAfter Mini-Batch Gradient Descent:\")\n",
    "print(\"Weights:\", w_mini)\n",
    "print(\"Bias:\", b_mini)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [0.05 0.1 ]\n",
      "Initial bias: -0.09\n",
      "\n",
      "--- Stochastic Gradient Descent ---\n",
      "Example 1: x = [1, 0], y = 1\n",
      "  z = -0.0400\n",
      "  Sigmoid (ŷ) = 0.49\n",
      "  Error (ŷ - y) = -0.51\n",
      "  Gradients: ∂L/∂w1 = -0.1274, ∂L/∂w2 = -0.0, ∂L/∂b = -0.1274\n",
      "  Updated Weights: w1 = 0.0627, w2 = 0.1000\n",
      "  Updated Bias: b = -0.0773\n",
      "\n",
      "Example 2: x = [0, 1], y = 1\n",
      "  z = 0.0227\n",
      "  Sigmoid (ŷ) = 0.5057\n",
      "  Error (ŷ - y) = -0.4943\n",
      "  Gradients: ∂L/∂w1 = -0.0, ∂L/∂w2 = -0.1236, ∂L/∂b = -0.1236\n",
      "  Updated Weights: w1 = 0.0627, w2 = 0.1124\n",
      "  Updated Bias: b = -0.0649\n",
      "\n",
      "Example 3: x = [0, 0], y = 0\n",
      "  z = -0.0649\n",
      "  Sigmoid (ŷ) = 0.4838\n",
      "  Error (ŷ - y) = 0.4838\n",
      "  Gradients: ∂L/∂w1 = 0.0, ∂L/∂w2 = 0.0, ∂L/∂b = 0.1208\n",
      "  Updated Weights: w1 = 0.0627, w2 = 0.1124\n",
      "  Updated Bias: b = -0.0770\n",
      "\n",
      "Example 4: x = [1, 1], y = 1\n",
      "  z = 0.0981\n",
      "  Sigmoid (ŷ) = 0.5245\n",
      "  Error (ŷ - y) = -0.4755\n",
      "  Gradients: ∂L/∂w1 = -0.1186, ∂L/∂w2 = -0.1186, ∂L/∂b = -0.1186\n",
      "  Updated Weights: w1 = 0.0746, w2 = 0.1242\n",
      "  Updated Bias: b = -0.0651\n",
      "\n",
      "Final Weights and Bias:\n",
      "Weights: [0.0746 0.1242]\n",
      "Bias: -0.0651\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Dataset\n",
    "D = [([1, 0], 1), ([0, 1], 1), ([0, 0], 0), ([1, 1], 1)]\n",
    "\n",
    "# Initial weights and bias\n",
    "w = np.array([0.05, 0.1])  # w1, w2\n",
    "b = -0.09  # bias\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.1\n",
    "\n",
    "# Compute gradients for one sample\n",
    "def compute_gradients(x, y, w, b):\n",
    "    z = np.dot(w, x) + b  # Linear combination\n",
    "    y_hat = sigmoid(z)    # Sigmoid activation\n",
    "    error = y_hat - y      # Error\n",
    "    # Gradients with respect to weights and bias\n",
    "    dw = error * y_hat * (1 - y_hat) * np.array(x)\n",
    "    db = error * y_hat * (1 - y_hat)\n",
    "    return np.round(dw, 4), round(db, 4), round(y_hat, 4), round(error, 4)\n",
    "\n",
    "# Stochastic Gradient Descent with Detailed Output\n",
    "def stochastic_gradient_descent(D, w, b, eta):\n",
    "    print(\"\\n--- Stochastic Gradient Descent ---\")\n",
    "    for i, (x, y) in enumerate(D):\n",
    "        dw, db, y_hat, error = compute_gradients(x, y, w, b)\n",
    "        print(f\"Example {i+1}: x = {x}, y = {y}\")\n",
    "        print(f\"  z = {np.dot(w, x) + b:.4f}\")\n",
    "        print(f\"  Sigmoid (ŷ) = {y_hat}\")\n",
    "        print(f\"  Error (ŷ - y) = {error}\")\n",
    "        print(f\"  Gradients: ∂L/∂w1 = {dw[0]}, ∂L/∂w2 = {dw[1]}, ∂L/∂b = {db}\")\n",
    "        # Update weights and bias\n",
    "        w -= eta * dw\n",
    "        b -= eta * db\n",
    "        print(f\"  Updated Weights: w1 = {w[0]:.4f}, w2 = {w[1]:.4f}\")\n",
    "        print(f\"  Updated Bias: b = {b:.4f}\\n\")\n",
    "    return np.round(w, 4), round(b, 4)\n",
    "\n",
    "# Run one iteration\n",
    "print(\"Initial weights:\", w)\n",
    "print(\"Initial bias:\", b)\n",
    "\n",
    "# Perform Stochastic Gradient Descent\n",
    "w_sgd, b_sgd = stochastic_gradient_descent(D, w.copy(), b, eta)\n",
    "\n",
    "print(\"Final Weights and Bias:\")\n",
    "print(\"Weights:\", w_sgd)\n",
    "print(\"Bias:\", b_sgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial W1: [[ 1.5  2.4  2. ]\n",
      " [-2.5  1.7  3.2]\n",
      " [ 2.8  1.1 -2. ]]\n",
      "Initial b1: [ 2.4  1.8 -3.5]\n",
      "Initial W2: [-2.3  1.7 -1.1]\n",
      "Initial b2: 2.8\n",
      "\n",
      "Hidden Layer Activations (a1): [0.9998 0.9852 0.168 ]\n",
      "Predicted Output (ŷ): 0.8798\n",
      "\n",
      "Gradients for Layer 1:\n",
      "dL/dW1: [[-0.0001 -0.0001 -0.0001]\n",
      " [ 0.0023  0.0023  0.0023]\n",
      " [-0.0143 -0.0143 -0.0143]]\n",
      "dL/db1: [-0.0001  0.0023 -0.0143]\n",
      "\n",
      "Gradients for Layer 2:\n",
      "dL/dW2: [0.093  0.0917 0.0156]\n",
      "dL/db2: 0.093\n",
      "\n",
      "Updated W1: [[ 1.5     2.4     2.    ]\n",
      " [-2.5012  1.6988  3.1988]\n",
      " [ 2.8072  1.1072 -1.9928]]\n",
      "Updated b1: [ 2.4     1.7988 -3.4928]\n",
      "Updated W2: [-2.3465  1.6542 -1.1078]\n",
      "Updated b2: 2.7535\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Parameters (from the problem)\n",
    "W1 = np.array([[1.5, 2.4, 2.0],\n",
    "               [-2.5, 1.7, 3.2],\n",
    "               [2.8, 1.1, -2.0]])  # Weights for layer 1\n",
    "\n",
    "b1 = np.array([2.4, 1.8, -3.5])  # Biases for layer 1\n",
    "\n",
    "W2 = np.array([-2.3, 1.7, -1.1])  # Weights for layer 2\n",
    "b2 = 2.8  # Bias for layer 2\n",
    "\n",
    "# Input and target output\n",
    "x = np.array([1, 1, 1])  # Input sample\n",
    "y = 0  # Target output\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.5\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(x, W1, b1, W2, b2):\n",
    "    # Layer 1\n",
    "    z1 = np.dot(W1, x) + b1  # Linear combination\n",
    "    a1 = sigmoid(z1)         # Activation (a1 = [a1_1, a1_2, a1_3])\n",
    "\n",
    "    # Layer 2\n",
    "    z2 = np.dot(W2, a1) + b2  # Linear combination\n",
    "    y_hat = sigmoid(z2)       # Activation (output)\n",
    "\n",
    "    return z1, a1, z2, y_hat\n",
    "\n",
    "# Backward pass and weight updates\n",
    "def backpropagation(x, y, W1, b1, W2, b2, eta):\n",
    "    # Forward pass\n",
    "    z1, a1, z2, y_hat = forward_pass(x, W1, b1, W2, b2)\n",
    "\n",
    "    # Output layer error and gradients\n",
    "    error_output = y_hat - y\n",
    "    dL_dz2 = error_output * sigmoid_derivative(z2)  # Gradient of loss w.r.t. z2\n",
    "\n",
    "    dL_dW2 = dL_dz2 * a1  # Gradient of loss w.r.t. W2\n",
    "    dL_db2 = dL_dz2       # Gradient of loss w.r.t. b2\n",
    "\n",
    "    # Hidden layer errors and gradients\n",
    "    dL_da1 = W2 * dL_dz2  # Error propagated to hidden layer\n",
    "    dL_dz1 = dL_da1 * sigmoid_derivative(z1)  # Gradient of loss w.r.t. z1\n",
    "\n",
    "    dL_dW1 = np.outer(dL_dz1, x)  # Gradient of loss w.r.t. W1\n",
    "    dL_db1 = dL_dz1               # Gradient of loss w.r.t. b1\n",
    "\n",
    "    # Update weights and biases\n",
    "    W1 -= eta * dL_dW1\n",
    "    b1 -= eta * dL_db1\n",
    "    W2 -= eta * dL_dW2\n",
    "    b2 -= eta * dL_db2\n",
    "\n",
    "    return W1, b1, W2, b2, a1, y_hat, dL_dW1, dL_db1, dL_dW2, dL_db2\n",
    "\n",
    "# Perform one iteration of backpropagation\n",
    "print(\"Initial W1:\", W1)\n",
    "print(\"Initial b1:\", b1)\n",
    "print(\"Initial W2:\", W2)\n",
    "print(\"Initial b2:\", b2)\n",
    "\n",
    "W1, b1, W2, b2, a1, y_hat, grad_W1, grad_b1, grad_W2, grad_b2 = backpropagation(x, y, W1, b1, W2, b2, eta)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nHidden Layer Activations (a1):\", np.round(a1, 4))\n",
    "print(\"Predicted Output (ŷ):\", np.round(y_hat, 4))\n",
    "\n",
    "print(\"\\nGradients for Layer 1:\")\n",
    "print(\"dL/dW1:\", np.round(grad_W1, 4))\n",
    "print(\"dL/db1:\", np.round(grad_b1, 4))\n",
    "\n",
    "print(\"\\nGradients for Layer 2:\")\n",
    "print(\"dL/dW2:\", np.round(grad_W2, 4))\n",
    "print(\"dL/db2:\", np.round(grad_b2, 4))\n",
    "\n",
    "print(\"\\nUpdated W1:\", np.round(W1, 4))\n",
    "print(\"Updated b1:\", np.round(b1, 4))\n",
    "print(\"Updated W2:\", np.round(W2, 4))\n",
    "print(\"Updated b2:\", np.round(b2, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Flatten the data\n",
    "x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "# 2. Define the Autoencoder\n",
    "encoding_dim = 32  # Size of the encoded representations\n",
    "\n",
    "# Input layer\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# Encoder\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# Encoder model for extracting features\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 3. Train the Autoencoder\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=20,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "# 4. Extract features from the encoder\n",
    "encoded_features = encoder.predict(x_train)\n",
    "\n",
    "# 5. Use KMeans for unsupervised classification\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(encoded_features)\n",
    "\n",
    "# Map KMeans cluster labels to actual digits\n",
    "def map_clusters_to_labels(kmeans_labels, true_labels):\n",
    "    label_mapping = {}\n",
    "    for cluster in np.unique(kmeans_labels):\n",
    "        cluster_indices = np.where(kmeans_labels == cluster)[0]\n",
    "        most_common_label = np.bincount(true_labels[cluster_indices]).argmax()\n",
    "        label_mapping[cluster] = most_common_label\n",
    "    return np.vectorize(label_mapping.get)(kmeans_labels)\n",
    "\n",
    "# Map the clusters to labels\n",
    "cluster_labels = map_clusters_to_labels(kmeans.labels_, y_train)\n",
    "\n",
    "# 6. Evaluate the unsupervised classification accuracy\n",
    "accuracy = accuracy_score(y_train, cluster_labels)\n",
    "print(f\"Unsupervised Classification Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# 7. Visualize original and reconstructed images\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10  # Number of digits to display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

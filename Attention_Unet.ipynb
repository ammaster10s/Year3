{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE6bipNWOZcB0R1lLJgAmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammaster10s/Year3/blob/main/Attention_Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjCR4K9m9IWW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge COCO file\n"
      ],
      "metadata": {
        "id": "cJdfbVdA9Og7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install pycocotools\n",
        "from pycocotools.coco import COCO\n",
        "import json\n",
        "\n",
        "def merge_coco_json(json_files, output_file):\n",
        "    merged_annotations = {\n",
        "        \"info\": {},\n",
        "        \"licenses\": [],\n",
        "        \"images\": [],\n",
        "        \"annotations\": [],\n",
        "        \"categories\": []\n",
        "    }\n",
        "\n",
        "    image_id_offset = 0\n",
        "    annotation_id_offset = 0\n",
        "    category_id_offset = 0\n",
        "    existing_category_ids = set()\n",
        "\n",
        "    for idx, file in enumerate(json_files):\n",
        "        coco = COCO(file)\n",
        "\n",
        "        # Update image IDs to avoid conflicts\n",
        "        for image in coco.dataset['images']:\n",
        "            image['id'] += image_id_offset\n",
        "            merged_annotations['images'].append(image)\n",
        "\n",
        "        # Update annotation IDs to avoid conflicts\n",
        "        for annotation in coco.dataset['annotations']:\n",
        "            annotation['id'] += annotation_id_offset\n",
        "            annotation['image_id'] += image_id_offset\n",
        "            merged_annotations['annotations'].append(annotation)\n",
        "\n",
        "        # Update categories and their IDs to avoid conflicts\n",
        "        for category in coco.dataset['categories']:\n",
        "            if category['id'] not in existing_category_ids:\n",
        "                category['id'] += category_id_offset\n",
        "                merged_annotations['categories'].append(category)\n",
        "                existing_category_ids.add(category['id'])\n",
        "\n",
        "        image_id_offset = len(merged_annotations['images'])\n",
        "        annotation_id_offset = len(merged_annotations['annotations'])\n",
        "        category_id_offset = len(merged_annotations['categories'])\n",
        "\n",
        "    # Save merged annotations to output file\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(merged_annotations, f)\n",
        "\n",
        "# List of paths to COCO JSON files to merge\n",
        "json_files = [\"/content/drive/MyDrive/Working/train/CNV500.json\", \"/content/drive/MyDrive/Working/train/instances_default.json\"]\n",
        "\n",
        "# Output file path for merged annotations\n",
        "output_file = \"merged_coco.json\"\n",
        "\n",
        "# Merge COCO JSON files\n",
        "merge_coco_json(json_files, output_file)\n",
        "\n",
        "print(\"Merged COCO JSON files saved to\", output_file)"
      ],
      "metadata": {
        "id": "vVn4wcv69QzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the data"
      ],
      "metadata": {
        "id": "24dnk4hU9Rly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from pycocotools.mask import decode\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image, ImageDraw\n",
        "from pycocotools import mask as maskUtils\n",
        "\n",
        "# Function to handle both RLE and polygon segmentation formats in COCO\n",
        "def segmentation_to_mask(segmentation, width, height):\n",
        "    if isinstance(segmentation, dict):  # RLE format (compressed RLE)\n",
        "        print(\"Decoding RLE mask\")\n",
        "        rle = maskUtils.frPyObjects(segmentation, height, width)\n",
        "        mask = maskUtils.decode(rle)\n",
        "        return mask\n",
        "    elif isinstance(segmentation, list):  # Polygon format\n",
        "        print(\"Processing polygon mask\")\n",
        "        mask = np.zeros((height, width), dtype=np.uint8)\n",
        "        # Create a binary mask from polygons\n",
        "        for polygon in segmentation:\n",
        "            poly = np.array(polygon).reshape((-1, 2))\n",
        "            img = Image.new('L', (width, height), 0)\n",
        "            ImageDraw.Draw(img).polygon([tuple(p) for p in poly], outline=1, fill=1)\n",
        "            mask = np.array(img, dtype=np.uint8)\n",
        "        return mask\n",
        "    else:\n",
        "        raise TypeError(\"Unsupported segmentation format\")\n",
        "\n",
        "# Function to parse the COCO JSON file and return annotations\n",
        "def parse_coco_json(json_file, image_dir):\n",
        "    with open(json_file, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "\n",
        "    annotations = []\n",
        "    image_data = {}\n",
        "\n",
        "    # Map image ID to file name and other metadata\n",
        "    for img_info in coco_data['images']:\n",
        "        image_data[img_info['id']] = {\n",
        "            'file_name': img_info['file_name'],\n",
        "            'width': img_info['width'],\n",
        "            'height': img_info['height']\n",
        "        }\n",
        "\n",
        "    # Extract segmentation and mask information\n",
        "    for ann in coco_data['annotations']:\n",
        "        image_id = ann['image_id']\n",
        "        image_info = image_data[image_id]\n",
        "        image_filename = image_info['file_name']\n",
        "        width = image_info['width']\n",
        "        height = image_info['height']\n",
        "\n",
        "        # Get the segmentation mask (COCO format supports RLE or polygons)\n",
        "        mask = segmentation_to_mask(ann['segmentation'], width, height)\n",
        "\n",
        "        annotations.append((image_filename, mask))\n",
        "\n",
        "    return annotations\n",
        "\n",
        "# Load image data and their masks, skip if no mask available\n",
        "def load_data_with_masks(image_dir, annotation_file, target_size=(256, 256)):\n",
        "    annotations = parse_coco_json(annotation_file, image_dir)\n",
        "    images, masks = [], []\n",
        "\n",
        "    for image_filename, mask in annotations:\n",
        "        image_path = os.path.join(image_dir, image_filename)\n",
        "        if mask is not None:\n",
        "            # Load image\n",
        "            img = img_to_array(load_img(image_path, color_mode='grayscale', target_size=target_size))\n",
        "            images.append(img)\n",
        "\n",
        "            # Resize mask to target size and append\n",
        "            mask_resized = np.array(Image.fromarray(mask).resize(target_size, Image.NEAREST))\n",
        "            masks.append(mask_resized)\n",
        "        else:\n",
        "            print(f\"Skipping {image_filename} due to invalid or missing mask\")\n",
        "\n",
        "    if len(images) == 0:\n",
        "        print(\"No valid images and masks were loaded!\")\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Paths to your dataset (Images and COCO annotations)\n",
        "train_image_dir = '/content/drive/MyDrive/Working/train/CNV'\n",
        "train_json_annotation_file = '/content/merged_coco.json'  # Training JSON file\n",
        "val_image_dir = '/content/drive/MyDrive/Working/val/CNV35'\n",
        "val_json_annotation_file = '/content/drive/MyDrive/Working/val/instances_default.json'  # Validation JSON file\n",
        "\n",
        "# Load the images and masks for training and validation\n",
        "X_train, Y_train = load_data_with_masks(train_image_dir, train_json_annotation_file, target_size=(256, 256))\n",
        "X_val, Y_val = load_data_with_masks(val_image_dir, val_json_annotation_file, target_size=(256, 256))\n",
        "\n",
        "# Check if there are valid data loaded\n",
        "if X_train.shape[0] == 0 or Y_train.shape[0] == 0:\n",
        "    raise ValueError(\"Training data is empty. Check your annotations and images.\")\n",
        "if X_val.shape[0] == 0 or Y_val.shape[0] == 0:\n",
        "    raise ValueError(\"Validation data is empty. Check your annotations and images.\")\n",
        "\n",
        "# Shuffle the training dataset (important!)\n",
        "train_indices = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(train_indices)\n",
        "X_train = X_train[train_indices]\n",
        "Y_train = Y_train[train_indices]\n",
        "\n",
        "# Shuffle the validation dataset\n",
        "val_indices = np.arange(X_val.shape[0])\n",
        "np.random.shuffle(val_indices)\n",
        "X_val = X_val[val_indices]\n",
        "Y_val = Y_val[val_indices]\n",
        "\n",
        "# Normalize pixel values to 0-1 range for images\n",
        "X_train = X_train / 255.0\n",
        "X_val = X_val / 255.0\n",
        "\n",
        "# Normalize masks (convert them to binary 0/1 masks)\n",
        "Y_train = (Y_train > 0).astype(np.uint8)\n",
        "Y_val = (Y_val > 0).astype(np.uint8)\n",
        "\n",
        "# Generate augmented images for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# No augmentation for validation, only rescale images\n",
        "val_datagen = ImageDataGenerator()\n",
        "\n",
        "# Create training and validation generators\n",
        "train_gen = train_datagen.flow(X_train, Y_train, batch_size=4)\n",
        "val_gen = val_datagen.flow(X_val, Y_val, batch_size=4)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PFZ8YrjM9WVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Configuration"
      ],
      "metadata": {
        "id": "ezT8OWnz9YkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, concatenate, Activation, BatchNormalization\n",
        "\n",
        "\n",
        "def repeat_elem(tensor, rep):\n",
        "    # lambda function to repeat Repeats the elements of a tensor along an axis\n",
        "    #by a factor of rep.\n",
        "    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape\n",
        "    #(None, 256,256,6), if specified axis=3 and rep=2.\n",
        "\n",
        "     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
        "                          arguments={'repnum': rep})(tensor)\n",
        "\n",
        "def attention_block(x, gating, inter_shape):\n",
        "    shape_x = K.int_shape(x)\n",
        "    shape_g = K.int_shape(gating)\n",
        "\n",
        "# Getting the x signal to the same shape as the gating signal\n",
        "    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n",
        "    shape_theta_x = K.int_shape(theta_x)\n",
        "\n",
        "# Getting the gating signal to the same number of filters as the inter_shape\n",
        "    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gating)\n",
        "    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),\n",
        "                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n",
        "                                 padding='same')(phi_g)  # 16\n",
        "\n",
        "    concat_xg = layers.add([upsample_g, theta_x])\n",
        "    act_xg = layers.Activation('relu')(concat_xg)\n",
        "    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)\n",
        "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
        "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
        "    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
        "\n",
        "    upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n",
        "\n",
        "    y = layers.multiply([upsample_psi, x])\n",
        "\n",
        "    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)\n",
        "    result_bn = layers.BatchNormalization()(result)\n",
        "    return result_bn\n",
        "\n",
        "def gating_signal(input, out_size, batch_norm=False):\n",
        "    \"\"\"\n",
        "    resize the down layer feature map into the same dimension as the up layer feature map\n",
        "    using 1x1 conv\n",
        "    :return: the gating feature map with the same dimension of the up layer feature map\n",
        "    \"\"\"\n",
        "    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)\n",
        "    if batch_norm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def conv_block(input, filter_size, num_filters, dropout_rate=0.0, batch_norm=True):\n",
        "    # A convolutional block with optional batch normalization and dropout\n",
        "    x = Conv2D(num_filters, (filter_size, filter_size), padding=\"same\")(input)\n",
        "    if batch_norm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Conv2D(num_filters, (filter_size, filter_size), padding=\"same\")(x)\n",
        "    if batch_norm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    if dropout_rate > 0:\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "GtjRasqS9bl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention U_NET model"
      ],
      "metadata": {
        "id": "2nFQrpW99cPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/bnsreenu/python_for_microscopists/blob/master/224_225_226_models.py#L78\n",
        "\n",
        "def Attention_UNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):\n",
        "    FILTER_NUM = 32  # number of filters for the first layer\n",
        "    FILTER_SIZE = 3  # size of the convolutional filter\n",
        "    UP_SAMP_SIZE = 2  # size of upsampling filters\n",
        "\n",
        "    inputs = layers.Input(input_shape, dtype=tf.float32)\n",
        "\n",
        "    # Encoder path\n",
        "    conv1 = conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = conv_block(pool1, FILTER_SIZE, 2 * FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = conv_block(pool2, FILTER_SIZE, 4 * FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = conv_block(pool3, FILTER_SIZE, 8 * FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = conv_block(pool4, FILTER_SIZE, 16 * FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    # Decoder path with attention\n",
        "    gating_4 = gating_signal(conv5, 8 * FILTER_NUM, batch_norm)\n",
        "    attn_4 = attention_block(conv4, gating_4, 8 * FILTER_NUM)\n",
        "    up4 = UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE))(conv5)\n",
        "    up4 = concatenate([up4, attn_4], axis=3)\n",
        "    up_conv4 = conv_block(up4, FILTER_SIZE, 8 * FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    gating_3 = gating_signal(up_conv4, 4 * FILTER_NUM, batch_norm)\n",
        "    attn_3 = attention_block(conv3, gating_3, 4 * FILTER_NUM)\n",
        "    up3 = UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE))(up_conv4)\n",
        "    up3 = concatenate([up3, attn_3], axis=3)\n",
        "    up_conv3 = conv_block(up3, FILTER_SIZE, 4 * FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    gating_2 = gating_signal(up_conv3, 2 * FILTER_NUM, batch_norm)\n",
        "    attn_2 = attention_block(conv2, gating_2, 2 * FILTER_NUM)\n",
        "    up2 = UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE))(up_conv3)\n",
        "    up2 = concatenate([up2, attn_2], axis=3)\n",
        "    up_conv2 = conv_block(up2, FILTER_SIZE, 2 * FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    gating_1 = gating_signal(up_conv2, FILTER_NUM, batch_norm)\n",
        "    attn_1 = attention_block(conv1, gating_1, FILTER_NUM)\n",
        "    up1 = UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE))(up_conv2)\n",
        "    up1 = concatenate([up1, attn_1], axis=3)\n",
        "    up_conv1 = conv_block(up1, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    # Output layer\n",
        "    conv_final = Conv2D(NUM_CLASSES, kernel_size=(1, 1), padding=\"same\")(up_conv1)\n",
        "    conv_final = Activation(\"sigmoid\")(conv_final)  # Use \"softmax\" for multiple classes\n",
        "\n",
        "    # Model integration\n",
        "    model = models.Model(inputs, conv_final, name=\"Attention_UNet\")\n",
        "    return model\n",
        "\n",
        "# Instantiate and compile the Attention U-Net model\n",
        "model = Attention_UNet(input_shape=(256, 256, 1))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary (optional)\n",
        "# attn_unet_model.summary()"
      ],
      "metadata": {
        "id": "MZEWnqku9e8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ehJRRdsA9h2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with input images and masks\n",
        "history = model.fit(\n",
        "    X_train, Y_train,  # Input: X_train (images), Output: Y_train (masks)\n",
        "    epochs=100,\n",
        "    batch_size=2,\n",
        "    validation_data=(X_val, Y_val),  # Validation images and corresponding masks\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "L9ltSp0Q9jJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "9XEGvg8n9lR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import json\n",
        "\n",
        "# Load the pretrained U-Net model\n",
        "# model_path = '/content/drive/MyDrive/Working/models/unet_model_62epoch_423.h5'\n",
        "# unet_model = load_model(model_path)\n",
        "\n",
        "# Load and preprocess the full image for prediction\n",
        "def load_full_image(image_path, target_size=(256, 256)):\n",
        "    try:\n",
        "        full_image = load_img(image_path, color_mode='grayscale', target_size=target_size)\n",
        "        full_image_array = img_to_array(full_image) / 255.0\n",
        "        full_image_array = np.expand_dims(full_image_array, axis=0)  # Add batch dimension\n",
        "        return full_image_array\n",
        "    except UnidentifiedImageError:\n",
        "        print(f\"Cannot identify image file: {image_path}. Skipping.\")\n",
        "        return None  # Return None if image cannot be loaded\n",
        "\n",
        "# Function to evaluate the model's prediction for normal images (no NV expected)\n",
        "def evaluate_normal_image_prediction(predicted_mask, threshold=0.5):\n",
        "    # Apply threshold to predicted mask\n",
        "    predicted_mask_binary = (predicted_mask > threshold).astype(np.uint8)\n",
        "\n",
        "    # If any positive pixels are detected, mark it as incorrect\n",
        "    if np.sum(predicted_mask_binary) > 0:\n",
        "        return 0  # Incorrect, since NV was detected in a normal image\n",
        "    else:\n",
        "        return 1  # Correct, since no NV was detected in a normal image\n",
        "\n",
        "# Function to evaluate the model on normal images (no ground truth mask available)\n",
        "def evaluate_model_on_normal_images(test_dir, threshold=0.5):\n",
        "    total_images = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    # Loop through all images in the directory\n",
        "    for image_filename in os.listdir(test_dir):\n",
        "        if image_filename.endswith('.jpeg') or image_filename.endswith('.jpg') or image_filename.endswith('.png'):\n",
        "            full_image_path = os.path.join(test_dir, image_filename)\n",
        "\n",
        "            # Load and preprocess the image\n",
        "            full_image = load_full_image(full_image_path)\n",
        "\n",
        "            if full_image is None:\n",
        "                continue  # Skip if image couldn't be loaded\n",
        "\n",
        "            # Predict the mask for the full image\n",
        "            predicted_mask = model.predict(full_image)[0, :, :, 0]  # Extract the 2D mask\n",
        "\n",
        "            # Evaluate the prediction for a normal image\n",
        "            is_correct = evaluate_normal_image_prediction(predicted_mask, threshold)\n",
        "\n",
        "            total_images += 1\n",
        "            total_correct += is_correct\n",
        "\n",
        "            # Display evaluation results\n",
        "            print(f\"{image_filename}: Prediction Correct: {is_correct}\")\n",
        "\n",
        "    # Print final accuracy\n",
        "    accuracy = total_correct / total_images if total_images > 0 else 0\n",
        "    print(f\"Total Correct: {total_correct}/{total_images}\")\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Paths to your dataset (Normal images without ground truth masks)\n",
        "test_image_dir = '/content/drive/MyDrive/Working/train/NORMAL'  # Directory for normal images (no NV expected)\n",
        "\n",
        "# Run the evaluation for normal images\n",
        "evaluate_model_on_normal_images(test_image_dir)\n"
      ],
      "metadata": {
        "id": "UgpUtvsv9miz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize"
      ],
      "metadata": {
        "id": "AnARz8Ij9nI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from PIL import Image\n",
        "from pycocotools import mask as maskUtils\n",
        "import json\n",
        "\n",
        "# Load the pretrained U-Net model\n",
        "model_path = '/content/drive/MyDrive/Working/models/attention_unet_model_62epoch_423.h5'\n",
        "unet_model = load_model(model_path)\n",
        "\n",
        "# Load and preprocess the full image for prediction\n",
        "def load_full_image(image_path, target_size=(256, 256)):\n",
        "    full_image = load_img(image_path, color_mode='grayscale', target_size=target_size)\n",
        "    full_image_array = img_to_array(full_image) / 255.0\n",
        "    full_image_array = np.expand_dims(full_image_array, axis=0)  # Add batch dimension\n",
        "    return full_image_array\n",
        "\n",
        "# Function to handle both RLE and polygon segmentation formats in COCO\n",
        "def segmentation_to_mask(segmentation, width, height):\n",
        "    if isinstance(segmentation, dict):  # RLE format (compressed RLE)\n",
        "        # print(\"Decoding RLE mask\")\n",
        "        rle = maskUtils.frPyObjects(segmentation, height, width)\n",
        "        mask = maskUtils.decode(rle)\n",
        "        return mask\n",
        "    elif isinstance(segmentation, list):  # Polygon format\n",
        "        # print(\"Processing polygon mask\")\n",
        "        mask = np.zeros((height, width), dtype=np.uint8)\n",
        "        # Create a binary mask from polygons\n",
        "        for polygon in segmentation:\n",
        "            poly = np.array(polygon).reshape((-1, 2))\n",
        "            img = Image.new('L', (width, height), 0)\n",
        "            ImageDraw.Draw(img).polygon([tuple(p) for p in poly], outline=1, fill=1)\n",
        "            mask = np.array(img, dtype=np.uint8)\n",
        "        return mask\n",
        "    else:\n",
        "        raise TypeError(\"Unsupported segmentation format\")\n",
        "\n",
        "# Function to parse the COCO JSON file and return annotations\n",
        "def parse_coco_json(json_file, image_dir):\n",
        "    with open(json_file, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "\n",
        "    annotations = []\n",
        "    image_data = {}\n",
        "\n",
        "    # Map image ID to file name and other metadata\n",
        "    for img_info in coco_data['images']:\n",
        "        image_data[img_info['id']] = {\n",
        "            'file_name': img_info['file_name'],\n",
        "            'width': img_info['width'],\n",
        "            'height': img_info['height']\n",
        "        }\n",
        "\n",
        "    # Extract segmentation and mask information\n",
        "    for ann in coco_data['annotations']:\n",
        "        image_id = ann['image_id']\n",
        "        image_info = image_data[image_id]\n",
        "        image_filename = image_info['file_name']\n",
        "        width = image_info['width']\n",
        "        height = image_info['height']\n",
        "\n",
        "        # Get the segmentation mask (COCO format supports RLE or polygons)\n",
        "        mask = segmentation_to_mask(ann['segmentation'], width, height)\n",
        "\n",
        "        annotations.append((image_filename, mask))\n",
        "\n",
        "    return annotations\n",
        "\n",
        "# Function to compute IoU (Intersection over Union)\n",
        "def compute_iou(predicted_mask, ground_truth_mask):\n",
        "    # Flatten masks to compute IoU on 1D arrays\n",
        "    predicted_mask = predicted_mask.flatten()\n",
        "    ground_truth_mask = ground_truth_mask.flatten()\n",
        "\n",
        "    # Compute IoU\n",
        "    intersection = np.sum(np.logical_and(predicted_mask, ground_truth_mask))\n",
        "    union = np.sum(np.logical_or(predicted_mask, ground_truth_mask))\n",
        "    iou = intersection / union if union != 0 else 0\n",
        "\n",
        "    return iou\n",
        "\n",
        "# Function to compute Dice coefficient\n",
        "def compute_dice(predicted_mask, ground_truth_mask):\n",
        "    # Flatten masks\n",
        "    predicted_mask = predicted_mask.flatten()\n",
        "    ground_truth_mask = ground_truth_mask.flatten()\n",
        "\n",
        "    # Compute Dice Coefficient\n",
        "    intersection = np.sum(np.logical_and(predicted_mask, ground_truth_mask))\n",
        "    dice = (2. * intersection) / (np.sum(predicted_mask) + np.sum(ground_truth_mask))\n",
        "\n",
        "    return dice\n",
        "\n",
        "# Function to evaluate the model on each image and mask from COCO annotations\n",
        "def evaluate_model_on_coco(test_dir, coco_json_file, threshold=0.5):\n",
        "    iou_scores = []\n",
        "    dice_scores = []\n",
        "    annotations = parse_coco_json(coco_json_file, test_dir)\n",
        "    counter = 0\n",
        "\n",
        "    # Loop through all images and masks from COCO annotations\n",
        "    for image_filename, ground_truth_mask in annotations:\n",
        "        full_image_path = os.path.join(test_dir, image_filename)\n",
        "\n",
        "        if not os.path.exists(full_image_path):\n",
        "            # print(f\"Image {image_filename} not found, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Load and preprocess the image\n",
        "        full_image = load_full_image(full_image_path)\n",
        "\n",
        "        # Predict the mask for the full image\n",
        "        predicted_mask = unet_model.predict(full_image)[0, :, :, 0]  # Extract the 2D mask\n",
        "\n",
        "        # Apply threshold to convert the predicted mask to binary\n",
        "        predicted_mask_binary = (predicted_mask > threshold).astype(np.uint8)\n",
        "\n",
        "        # Resize the ground truth mask to match the predicted mask size\n",
        "        ground_truth_mask_resized = np.array(Image.fromarray(ground_truth_mask).resize((256, 256), Image.NEAREST))\n",
        "\n",
        "        # Calculate IoU and Dice scores\n",
        "        iou = compute_iou(predicted_mask_binary, ground_truth_mask_resized)\n",
        "        dice = compute_dice(predicted_mask_binary, ground_truth_mask_resized)\n",
        "\n",
        "        iou_scores.append(iou)\n",
        "        dice_scores.append(dice)\n",
        "\n",
        "        # Display evaluation results\n",
        "        print(f\"{image_filename}: IoU = {iou}, Dice Coefficient = {dice}\")\n",
        "\n",
        "        # Optionally plot every 10th result\n",
        "        # Optionally plot every 10th result\n",
        "        if counter % 10 == 0:\n",
        "            plt.figure(figsize=(16, 8))  # Adjusting the figure size for four subplots\n",
        "\n",
        "            # Subplot 1: Display the original full image\n",
        "            plt.subplot(1, 4, 1)\n",
        "            plt.imshow(np.squeeze(full_image), cmap='gray')\n",
        "            plt.title(f'Full Image: {image_filename}')\n",
        "\n",
        "            # Subplot 2: Display the predicted NV mask heatmap\n",
        "            plt.subplot(1, 4, 2)\n",
        "            plt.imshow(predicted_mask, cmap='jet', alpha=0.6)\n",
        "            plt.title('Predicted NV Heatmap')\n",
        "            plt.colorbar()\n",
        "\n",
        "            # Subplot 3: Overlay predicted mask on the full image\n",
        "            plt.subplot(1, 4, 3)\n",
        "            plt.imshow(np.squeeze(full_image), cmap='gray')  # Original image\n",
        "            plt.imshow(predicted_mask_binary, cmap='jet', alpha=0.5)  # Predicted mask overlay\n",
        "            plt.title('Overlay: Full Image with Predicted Mask')\n",
        "\n",
        "            # Subplot 4: Display the ground truth mask\n",
        "            plt.subplot(1, 4, 4)\n",
        "            plt.imshow(ground_truth_mask_resized, cmap='gray')\n",
        "            plt.title('Ground Truth Mask')\n",
        "\n",
        "            # Show the plots\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "    # Print average metrics\n",
        "    print(f\"Average IoU: {np.mean(iou_scores)}\")\n",
        "    print(f\"Average Dice Coefficient: {np.mean(dice_scores)}\")\n",
        "\n",
        "# Paths to your dataset (Images and COCO annotations)\n",
        "test_image_dir = '/content/drive/MyDrive/Working/train/CNV'\n",
        "test_json_annotation_file = '/content/drive/MyDrive/Working/train/instances_default.json'  # COCO JSON file with annotations\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_model_on_coco(test_image_dir, test_json_annotation_file)\n"
      ],
      "metadata": {
        "id": "lTbbiVLn9rEi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}